{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Transform' from 'net.network' (/public/dehezhang2/github/SAVA_Official_Implementation/net/network.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e764ee8fdf8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttentionNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSelfAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madaptive_instance_normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Transform' from 'net.network' (/public/dehezhang2/github/SAVA_Official_Implementation/net/network.py)"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from net.models import SAVA_test as SAVA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from net.network import AttentionNet, Encoder, SelfAttention, Transform, vgg, decoder\n",
    "from net.utils import adaptive_instance_normalization\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Basic options\n",
    "# parser.add_argument('--content_image', type=str, default='./test_result/recon/lenna_recon_adain_content_wct.png',\n",
    "#                     help='Directory path to a batch of content images')\n",
    "parser.add_argument('--content_image', type=str, default='./datasets/test/content/lenna.jpg',\n",
    "                    help='Directory path to a batch of content images')\n",
    "parser.add_argument('--style_image', type=str, default='./datasets/test/void_style/xile.jpg',\n",
    "                    help='Directory path to a batch of style images')\n",
    "\n",
    "parser.add_argument('--attn_model', type=str, default='models/content_wct/',\n",
    "                    help='Directory path to a batch of style images')\n",
    "parser.add_argument('--decoder_model', type=str, default='models/SANET/',\n",
    "                    help='Directory path to a batch of style images')\n",
    "parser.add_argument('--transform_model', type=str, default='models/SANET/',\n",
    "                    help='Directory path to a batch of style images')\n",
    "parser.add_argument('--vgg_model', type=str, default='models/SANET/',\n",
    "                    help='Directory path to a batch of style images')\n",
    "# parser.add_argument('--save_dir', type=str, default='test_results/',\n",
    "#                     help='Directory path to a batch of style images')\n",
    "args = parser.parse_args('')\n",
    "\n",
    "content_name = args.content_image.split('/')[-1].split('.')[0]\n",
    "style_name = args.style_image.split('/')[-1].split('.')[0]\n",
    "attn_model_dir = args.attn_model.split('/')[-2]\n",
    "decoder_model_dir = args.decoder_model.split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "_R_MEAN = 123.68\n",
    "_G_MEAN = 116.78\n",
    "_B_MEAN = 103.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test data\n",
    "\n",
    "def test_transform(size = 512):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size=(size, size)),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((_R_MEAN/255.0, _G_MEAN/255.0, _B_MEAN/255.0), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def test_transform_inv():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Normalize((0, 0, 0), (2, 2, 2)),\n",
    "        transforms.Normalize((-_R_MEAN/255.0, -_G_MEAN/255.0, -_B_MEAN/255.0), (1, 1, 1))\n",
    "    ])\n",
    "    return transform\n",
    "# scale = 1.40625\n",
    "content_tf = test_transform(int(512))\n",
    "style_tf = test_transform(int(512))\n",
    "content_tf_inv = test_transform_inv()\n",
    "content = content_tf(Image.open(args.content_image).convert('RGB'))\n",
    "print(content.shape)\n",
    "\n",
    "style = style_tf(Image.open(args.style_image).convert('RGB'))\n",
    "print(style.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "imshow(torchvision.utils.make_grid(content_tf_inv(content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(content_tf_inv(style)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder\n",
    "decoder.load_state_dict(torch.load(args.decoder_model + 'decoder_iter_500000.pth'))\n",
    "attn = SelfAttention()\n",
    "attn.load_state_dict(torch.load(args.attn_model + 'attention_kernel_iter_80000.pth'))\n",
    "transform = Transform(in_planes=512)\n",
    "transform.load_state_dict(torch.load(args.transform_model + 'transformer_iter_500000.pth'))\n",
    "vgg = vgg\n",
    "vgg.load_state_dict(torch.load(args.vgg_model + 'vgg_normalised.pth'))\n",
    "encoder = Encoder(encoder = vgg)\n",
    "\n",
    "state_dict = decoder.state_dict()\n",
    "for key in state_dict.keys():\n",
    "    state_dict[key] = state_dict[key].to(device)\n",
    "state_dict = attn.state_dict()\n",
    "for key in state_dict.keys():\n",
    "    state_dict[key] = state_dict[key].to(device)\n",
    "state_dict = transform.state_dict()\n",
    "for key in state_dict.keys():\n",
    "    state_dict[key] = state_dict[key].to(device)\n",
    "state_dict = vgg.state_dict()\n",
    "for key in state_dict.keys():\n",
    "    state_dict[key] = state_dict[key].to(device)\n",
    "\n",
    "transform.cuda()\n",
    "\n",
    "model = AttentionNet(attn=attn, encoder = encoder, decoder=decoder)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = torch.stack([content], dim = 0)\n",
    "content = content.to(device)\n",
    "style = torch.stack([style], dim = 0)\n",
    "style = style.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sava = SAVA(model, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, content_attn_map, style_attn_map= sava.transfer(content, style)\n",
    "output = adaptive_instance_normalization(output, style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_recon(img, save_dir):\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.savefig(save_dir)\n",
    "#     torchvision.utils.save_image(img, save_dir)\n",
    "    \n",
    "def show_output(output, save_dir):\n",
    "    print(\"Saving to \" + save_dir)\n",
    "    content_result = content_tf_inv(output.squeeze())\n",
    "    imshow_recon(torchvision.utils.make_grid(content_result.cpu()), save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_output(output, 'test_result/output/' + content_name + '_with_' + style_name + '_' + attn_model_dir + '_' + decoder_model_dir + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_attn_map(attn_map, save_dir):\n",
    "    channel_num = attn_map.size()[1]\n",
    "    mean_sal = torch.mean(attn_map, 1, False)\n",
    "    mean_sal_np = mean_sal.cpu().detach().numpy()\n",
    "    mean_sal_np = mean_sal_np - np.min(mean_sal_np)\n",
    "    mean_sal_np = mean_sal_np * 1.0 / np.max(mean_sal_np)\n",
    "    # print(mean_sal.size())\n",
    "    plt.imshow(mean_sal_np[0], cmap=cm.get_cmap('rainbow', 1000))\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "#     plt.savefig(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_attn_map(content_attn_map, 'test_result/attention/' + content_name + '_attn_' + attn_model_dir + '_' + decoder_model_dir + '.png')\n",
    "print(content_attn_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_attn_map(style_attn_map, 'test_result/attention/' + style_name + '_attn_' + attn_model_dir + '_' + decoder_model_dir + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
